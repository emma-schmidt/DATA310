# Slice of Data Science - Claire Heinbaugh

In her Slice of Data Science talk, Claire gave a fantastic presentation on transfer learning, stacked generalization, and Keras. Starting out with an overview of neural networks, she used a project she did using sonic variables to predict genres of music. I thought this was a great introduction to neural networks and training/testing splits of data - her example of this was if they had data on 100 songs, she might use 80 of them to train the network (providing the neural network with both the sonic variables and the “answer” aka the genre), and then see how well that model could predict the remaining 20 songs. 

After this introduction, things got more complicated - and even more interesting. I had never thought of using existing models as a jumping off point for creating my own models. Claire explained that Keras has a bunch of transfer learning models that are already trained on large datasets (of images, for example) and that you can use these models in your own analysis. I can see how this would be really useful for a project like we did at the beginning of the semester where we trained a model to “read” handwritten numbers. Instead of starting our model with random weights for different features, we would start out with weights that are close to what we would want (assuming the transfer learning model we used had been trained on something similar). Depending on how close we think the weights are to what we are looking for, Claire explained that we can choose to “freeze” some - or all - of the weights. This process can help to avoid overfitting. 

In addition to learning about transfer learning, she introduced me to stacked generalization. The example she gave is that if three people are looking at a blurry image and trying to guess what it actually is, they might have three different answers, and they likely got to those answers by thinking in different ways. This is analogous to different models using different algorithms to categorize an image. Using stacked generalization, we can take those three “guesses” as inputs, and train a model that weights each model’s “guess”. This is super cool, and not something I had thought of before. I will definitely keep it in mind as I progress through this course and start thinking about a final project.

All in all, I’m really glad I was able to make it to this Slice of Data Science! Claire was a great presenter, and gave some great tips and tricks for using python (and specifically Keras) to make better models. 
